{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.lib.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6 (Monday), AST 8581 / PHYS 8581 / CSCI 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>, Michael Steinbach <stei0062@umn.edu>, Nico Adams adams900@umn.edu\n",
    "\n",
    "\n",
    "With contributions totally ripped off from Gautham Narayan (UIUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Term Feedback Takeaways\n",
    "\n",
    "### Homework / Mid-term\n",
    "\n",
    "* There was a lot of encouragement to keep explaining as much as possible on homeworks, with clear improvement from the first few to the last few homeworks. We are very much trying to improve along these lines, including providing examples of what the final plots are to look like.\n",
    "\n",
    "* On a related note, the mid-term next week is the exact same format as the homeworks, but instead they are to be done without help from your colleagues. We aim to make it a bit easier than a standard homework, given this fact. \n",
    "* *We are planning on turning next Wednesday's class into office hours where we will be available to meet with us about questions you have.* This is to help with the potential flow of questions at the usual office hours.\n",
    "\n",
    "### In-class\n",
    "\n",
    "* A *wide* variety of opinions on class structure: some people want more lecture, some more in-class warm-ups and some more time for the breakouts. This is hard and not an obvious right answer here. As we transition into machine learning (which starts next week), we expect more pedagogical lectures from Michael S. to improve the theory /statistics presentation beyond what we have gotten so far.\n",
    "\n",
    "* We agree with the need for more introductory astronomy explanations; this will be a focus when prepping future lectures. \n",
    "\n",
    "* To support the HW better, we will try to provide stripped down examples of useful functions / libraries (like corner.py) to help introduce their use.\n",
    "\n",
    "### Textbook\n",
    "\n",
    "* Lack of a textbook also a concern. *Probably* next year we might try to Frankenstein a book together from available resources (although that becomes spendy :(. We will try to include as much foundational material as we can in the lectures for now (and point to free resources when we know of them / available).\n",
    "\n",
    "### Final-Project\n",
    "\n",
    "* To be clear, outside of some preparation (like soliciting ideas for the final projects, please do fill out here: https://forms.gle/RYugP8fdo22bo49M9) and perhaps a bit of preliminary reading, we do not plan to have overlap between the homeworks/mid-terms and the final project, which is set to begin a few weeks before the end of the course. \n",
    "\n",
    "* Once we have the topics, we will send out a form in which you will rank projects in order of preference, and will try to balance the groups by interest and corresponding skill sets from the self-assessment. We will include a space for requests for people you would like in your groups (but understand this is constrained by the previous requirement).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where do we stand?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Last Class: Lomb Scargle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pros:\n",
    "- Works with unevenly sampled data, and binning in phase helps build statistics even with noisy data\n",
    "\n",
    "Cons:\n",
    "- ANY PERIODIC SIGNAL IN THE DATA WILL EXHIBIT SOME POWER\n",
    "    - This includes aliases of the true period\n",
    "- Multiband extension requires  that the period/frequency is the same across all channels/passbands \n",
    "    - this is not the case for many time-series phenomena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today: How do check if the object is even variable? Will try out emcee for sampling...\n",
    "\n",
    "When dealing with time series data, the first thing that we want to know is if the system that we are studying is even variable (otherwise, there is no point doing time series analysis!).  \n",
    "\n",
    "In the context of frequentist statistics, this is a question of whether our data would have been obtained by chance if the no-variability null hypothesis were correct.\n",
    "\n",
    "If we find that our source *is* variable, then our time-series analysis has two main goals:\n",
    "1. Characterize the temporal correlation between different values of $y$ (i.e., characterize the \"light curve\").  For example by learning the parameters for a model.\n",
    "2. Predict future values of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the errors are known and Gaussian, we can simply compute $\\chi^2$ and the corresponding $p$ values for variation in a signal.\n",
    "\n",
    "For a sinusoidal variable signal\n",
    "\n",
    "# $$y(t) = A \\sin(\\omega t) + \\epsilon(t)$$\n",
    "\n",
    "where $\\epsilon(t)$ is normally distributed with mean 0 and constant errors, $\\sigma$, then the variance is\n",
    "\n",
    "# $$V = \\sigma^2 + A^2/2$$\n",
    "\n",
    "since $variance(sin(\\omega t)) = A^2/2$ and is not dependent on $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# If $A=0$ (no variability)\n",
    "* ### $\\sum_j \\left(\\frac{y_j}{\\sigma}\\right)^2 $ is $\\chi^2$ with $N$ degrees of freedom, mean $N$ and variance $2N$\n",
    "* ### Thus, $\\chi^2_{\\rm dof}=\\frac{1}{N} \\sum_j \\left(\\frac{y_j}{\\sigma}\\right)^2$  has an expectation value of 1 and a std dev  of  $\\sqrt{\\frac{2N}{N^2}} = \\sqrt\\frac{2}{N}$\n",
    "\n",
    "# If $|A|>0$ (variability)\n",
    "* ### The expection of $\\chi^2_{\\rm dof}$ will be larger than 1.\n",
    "* ### probability that $\\chi^2_{\\rm dof}>1 + 3 \\sqrt{2/N}$  is about 1 in 1000 (i.e., $>3\\sigma$ above 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If this **false-positive rate** is acceptable (even without variability, 1 times in 1000 we will observer a $\\chi^2_{\\rm dof}$ above this threshold) then the **minimum detectable amplitude** is:\n",
    "\n",
    "### $$A > \\left( \\frac{72}{N} \\right)^\\frac{1}{4} \\sigma \\approx 2.9 \\sigma N^{-{1 \\over 4}}$$\n",
    "\n",
    "which follows from equating $V/\\sigma^2=1 + 3 \\sqrt{2/N}$, expanding $V$, and solving for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It takes $N > 70$ to reach $A=\\sigma$.\n",
    "\n",
    "For $N=100$ data points, the minimum detectable amplitude is $A=0.92\\sigma$\n",
    "\n",
    "For $N=1000$, $A = 0.52\\sigma$ \n",
    "\n",
    "That is, **if we have enough observations, we can actually detect variability whose amplitude is smaller than the uncertainty in the measurements.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that this is the best that we can do under the assumption of the null hypothesis of no variability.  \n",
    "\n",
    "If instead we know the model (not limited to periodic variability), then we can perform a [**\"matched filter\"**](https://en.wikipedia.org/wiki/Matched_filter) analysis and improve upon this (i.e., we can positively identify lower-amplitude variability).  \n",
    "\n",
    "**Indeed in a Bayesian analysis, we must specify a model.**\n",
    "\n",
    "For non-periodic variability, the system can either be **stochastic** (like the stock market) or **temporally localized** (such as a flare/burst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Estimation, Model Selection, and Classification\n",
    "\n",
    "Time series analysis can be conducted in either the time domain or the frequency domain. We'll use all the same machinery we worked with in the first half of class. \n",
    "\n",
    "We can fit a model to $N$ data points $(t_i,y_i)$:\n",
    "\n",
    "# $$y_i(t_i) = \\sum_{m=1}^M \\beta_m T_m(t_i|\\theta_m) + \\epsilon_i,$$\n",
    "\n",
    "where the functions, $T_m$, do not need to be periodic, $t_i$ does not need to be evenly sampled and $\\theta_m$ are the model parameters.\n",
    "\n",
    "So, for example, if we have\n",
    "\n",
    "# $$y_i(t_i) = a \\sin(\\omega_0 t_i) + b \\cos (\\omega_1 t_i),$$\n",
    "\n",
    "then $a=\\beta_0$, $b=\\beta_1$, $\\omega_0=\\theta_0$, and $\\omega_1 = \\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Determining if you have the right model can also use the same machinery as we've used (posterior predictive checks, information criterion):\n",
    "\n",
    "Common deterministic models include\n",
    "\n",
    "# $$T(t) = \\sin(\\omega t)$$\n",
    "\n",
    "and\n",
    "\n",
    "# $$T(t) = \\exp(-\\alpha t),$$\n",
    "\n",
    "where the frequency, $\\omega$, and decay rate, $\\alpha$, are parameters to be estimated from the data.\n",
    "\n",
    "You might also consider a \"chirp\" signal with\n",
    "\n",
    "# $$T(t) = \\sin(\\phi + \\omega t + \\alpha t^2).$$\n",
    "\n",
    "(another way of thinking of a chirp is that the *frequency varies with time*; $\\omega_{\\rm instantaneous} = \\omega + \\alpha t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temporally Localized Signals ($\\S$ 10.4)\n",
    "\n",
    "Let's begin with the case of a stationary signal with an event localized in time.\n",
    "An example would be the signature of a [gravitational wave from LIGO](https://www.ligo.caltech.edu/news/ligo20160615).\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "            <td><img src=\"figures/small_multiples.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case we know the expected shape of the signal and the noise properties are understood, so we can do what is called *forward modeling*.  Specifically, we will identify the signal by using a **matched filter** (with MCMC to search for the parameter covariances).\n",
    "\n",
    "Even if we didn't know the shape of the distribution, we could use a non-parametric form to perform matched filter analysis.  Furthermore, for complex signals we can marginalize over \"nuisance\" parameters (e.g. start time or phase) that are not important for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling a time-series when we have a forward model\n",
    "\n",
    "### In the first half of this class, we were able to write down a nice parametric model for our observations\n",
    "\n",
    "### However, precisely because time is an exogenous variable - just a counter that we happen to record along with our measurements - we can't usually do that anymore. \n",
    "\n",
    "### A notable exception is for periodic phenomena, where while the time stamp is not important, there are characteristic time-scale that we can use to build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The best example of this is radial velocity experiments used to detect exoplanets\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "            <td><img src=\"figures/Radial-Velocity-Method-star-orbits.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Here, the behavior of the system is governed by Kepler's laws and there is a honest-to-goodness parametric model to describe the observations. \n",
    "\n",
    "For understanding the orbital equations, the following webpage is useful:\n",
    "https://www.classe.cornell.edu/~seb/celestia/orbital-parameters.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### You can write down simple models to describe the milk production of cows, and while it is true that any cows you launch will also obey Kepler's laws, there's no fundamental physics that governs their milk production (which is likely to cease in short order if you do actually test bovine adherence to Kepler's laws). \n",
    "\n",
    "### While the radial velocity method is expensive, because we can build very high resolution spectrographs (remember these are just devices that are mapping energy differences into angular separation), they're likely our best hope for finding an Earth-like exoplanet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMCEE: A cascaded affine invariant ensemble MCMC sampler. \"The MCMC hammer\"\n",
    "\n",
    "This is an implementation of the Goodman and Weare 2010 Affine invariant ensemble Markov Chain Monte Carlo (MCMC) sampler. The problem with many traditional MCMC samplers is that they can have slow convergence for badly scaled problems, and that it is difficult to optimize the random walk for high-dimensional problems.\n",
    "This is where the GW-algorithm really excels as it is affine invariant. It can achieve much better convergence on badly scaled problems. It is much simpler to get to work straight out of the box, and for that reason it truly deserves to be called the MCMC hammer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Exercise - Looking for planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn; seaborn.set() #nice plot formatting\n",
    "import pandas as pd\n",
    "import corner\n",
    "from collections import namedtuple\n",
    "from scipy import optimize\n",
    "from gatspy.periodic import LombScargleFast\n",
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "data = pd.read_csv('data/47UrsaeMajoris.txt', delim_whitespace=True)\n",
    "t, rv, rv_err = data.values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "Visualize this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Model\n",
    "\n",
    "The first important step is to define a mathematical (and computational) model of how the parameters of interest are reflected in our observations.\n",
    "\n",
    "Some references relating to what we're going to compute below:\n",
    "\n",
    "- Balan 2009: http://adsabs.harvard.edu/abs/2009MNRAS.394.1936B\n",
    "- Exofit Manual: http://www.star.ucl.ac.uk/~lahav/ExoFitv2.pdf\n",
    "- Hou 2014: http://arxiv.org/pdf/1401.6128.pdf\n",
    "\n",
    "The equation for radial velocity is this:\n",
    "\n",
    "$$\n",
    "v(t) = V - K[ \\sin(f + \\omega) + e \\sin(\\omega)]\n",
    "$$\n",
    "\n",
    "where $V$ is the overall velocity of the system, and\n",
    "\n",
    "$$\n",
    "K = \\frac{m_p}{m_s + m_p} \\frac{2\\pi}{T}\\frac{a \\sin i}{\\sqrt{1 - e^2}}\n",
    "$$\n",
    "\n",
    "The true anomaly $f$ satisfies\n",
    "\n",
    "$$\n",
    "\\cos(f) = \\frac{\\cos(E) - e}{1 - e\\cos E}\n",
    "$$\n",
    "\n",
    "Rearranging this we can write\n",
    "$$\n",
    "f = 2 \\cdot{\\rm atan2}\\left(\\sqrt{1 + e}\\sin(E/2), \\sqrt{1 - e} \\cos(E/2)\\right)\n",
    "$$\n",
    "\n",
    "The eccentric anomaly $E$ satisfies\n",
    "$$\n",
    "M = E - e\\sin E\n",
    "$$\n",
    "\n",
    "and the mean anomaly is\n",
    "$$\n",
    "M = \\frac{2\\pi}{T}(t + \\tau)\n",
    "$$\n",
    "\n",
    "and $\\tau$ is the time of pericenter passage, which we'll parametrize with the parameter $\\chi = \\tau /  T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are the parameters needed to compute the radial velocity:\n",
    "\n",
    "- $T$: orbital period\n",
    "- $K$: amplitude of RV oscillation\n",
    "- $V$: secular offset of RV oscillation\n",
    "- $e$: eccentricity\n",
    "- $\\omega$: longitude of periastron\n",
    "- $\\chi$: dimensionless phase offset\n",
    "\n",
    "Additionally, we will fit a scatter parameter $s$ which accounts for global data errors not reflected in the reported uncertainties (this is very similar to the third parameter from the linear fit we saw earlier)\n",
    "\n",
    "For convenience, we'll store these parameters in a ``namedtuple``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "params = namedtuple('params', ['T', 'e', 'K', 'V', 'omega', 'chi', 's'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### These just define the model we wrote down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "@np.vectorize\n",
    "def compute_E(M, e):\n",
    "    \"\"\"Solve Kepler's eqns for eccentric anomaly given mean anomaly\"\"\"\n",
    "    f = lambda E, M=M, e=e: E - e * np.sin(E) - M\n",
    "    return optimize.brentq(f, 0, 2 * np.pi)\n",
    "\n",
    "\n",
    "def radial_velocity(t, theta):\n",
    "    \"\"\"Compute radial velocity given orbital parameters\"\"\"\n",
    "    T, e, K, V, omega, chi = theta[:6]\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Likelihood and Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "theta_lim = params(T=(0.2, 2000),\n",
    "                   e=(0, 1),\n",
    "                   K=(0.01, 2000),\n",
    "                   V=(-2000, 2000),\n",
    "                   omega=(0, 2 * np.pi),\n",
    "                   chi=(0, 1),\n",
    "                   s=(0.001, 100))\n",
    "theta_min, theta_max = map(np.array, zip(*theta_lim))\n",
    "\n",
    "def log_prior(theta):\n",
    "    # UNIFORM PRIOR FOR ALL PAREMETERS\n",
    "\n",
    "def log_likelihood(theta, t, rv, rv_err):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def log_posterior(theta, t, rv, rv_err):\n",
    "    ln_prior = log_prior(theta)\n",
    "    if np.isinf(ln_prior):\n",
    "        return ln_prior\n",
    "    else:\n",
    "        return ln_prior + log_likelihood(theta, t, rv, rv_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Now use LombScargle output as initial guess and sample the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def make_starting_guess(t, rv, rv_err):\n",
    "    model = LombScargleFast()\n",
    "    model.optimizer.set(period_range=theta_lim.T,\n",
    "                        quiet=True)\n",
    "    model.fit(t, rv, rv_err)\n",
    "\n",
    "    rv_range = 0.5 * (np.max(rv) - np.min(rv))\n",
    "    rv_center = np.mean(rv)\n",
    "    return params(T=model.best_period,\n",
    "                  e=0.1,\n",
    "                  K=rv_range,\n",
    "                  V=rv_center,\n",
    "                  omega=np.pi,\n",
    "                  chi=0.5,\n",
    "                  s=rv_err.mean())\n",
    "\n",
    "theta_guess = make_starting_guess(t, rv, rv_err)\n",
    "theta_guess\n",
    "\n",
    "# Run emcee\n",
    "# Read https://emcee.readthedocs.io/en/stable/ for example\n",
    "\n",
    "ndim = len(theta_guess)  # number of parameters in the model\n",
    "nwalkers = 50  # number of MCMC walkers\n",
    "\n",
    "# start with a tight distribution of theta around the initial guess\n",
    "rng = np.random.RandomState(42)\n",
    "starting_guesses = theta_guess * (1 + 0.1 * rng.randn(nwalkers, ndim))\n",
    "\n",
    "# RUN MCMC - YOUR CODE HERE\n",
    "# sampler = emcee.EnsembleSampler(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a little function to plot the chains\n",
    "def plot_chains(sampler):\n",
    "    fig, ax = plt.subplots(ndim, figsize=(8, 10), sharex=True)\n",
    "    for i in range(ndim):\n",
    "        ax[i].plot(sampler.chain[:, :, i].T, '-k', alpha=0.2);\n",
    "        ax[i].set_ylabel(params._fields[i])\n",
    "\n",
    "        \n",
    "# PLOT YOUR CHAINS - YOUR CODE HERE\n",
    "plot_chains(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we've seen before the initial steps in the chain aren't useful samples and it takes a while to reach a stationary distribution. Notice though that the chains are *NOT* all mixed - there are some that are wandering away from the the remaining traces, and being stubborn about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN A FULL MCMC HERE RESETING THE SAMPLER AND RESTARTING FROM THE LAST POSITION AFTER THE BURN-IN\n",
    "# YOUR CODE HERE\n",
    "sampler.reset()\n",
    "pos, prob, state = sampler.run_mcmc(pos, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT YOUR CHAINS AGAIN - YOUR CODE HERE\n",
    "plot_chains(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND LOOK AT THE PERIOD AND ECCENTRICITY \n",
    "# WE CAN'T GET MASSES FROM THE RADIAL VELOCITY METHOD, \n",
    "# BUT IF WE CARE ABOUT HABITABILITY, LOOKING AT e IS A GOOD IDEA\n",
    "# Use corner.py or similar to look at the histograms\n",
    "corner.corner(sampler.flatchain[:, :2], labels=params._fields[:2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS TO VISUALIZE THE MODEL AGREEMENT WITH THE DATA\n",
    "t_fit = np.linspace(t.min(), t.max(), 1000)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# OVERPLOT THE ORIGINAL DATA WITH YOUR FIT\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.errorbar(t, rv, rv_err, fmt='.k')\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('radial velocity (km/s)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS TO GET A SENSE OF THE PERIOD AND ECCENTRICITY\n",
    "mean = sampler.flatchain.mean(0)\n",
    "std = sampler.flatchain.std(0)\n",
    "\n",
    "print(\"Period       = {0:.0f} +/- {1:.0f} days\".format(mean[0], std[0]))\n",
    "print(\"eccentricity = {0:.2f} +/- {1:.2f}\".format(mean[1], std[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
